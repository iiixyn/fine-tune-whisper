{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment set-up"
      ],
      "metadata": {
        "id": "525fuPPjMEZa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT9S0kg0Lt8G",
        "outputId": "957b683e-6f68-4235-9886-26e66ff32346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!add-apt-repository -y ppa:jonathonf/ffmpeg-4\n",
        "!apt update\n",
        "!apt install -y ffmpeg"
      ],
      "metadata": {
        "id": "WaJKW7EpNOnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install pytorch-lightning==1.7.7\n",
        "!pip install -qqq evaluate==0.2.2\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install ray[tune]"
      ],
      "metadata": {
        "id": "ejsL_pAoNRv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u4LFkDi_NVtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "4STKX71OOfN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display\n",
        "from pathlib import Path\n",
        "import whisper\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow \n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import torchaudio\n",
        "import torchaudio.transforms as at\n",
        "\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "import sklearn.model_selection\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import evaluate\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
        "from ray import air, tune"
      ],
      "metadata": {
        "id": "NsfdQxujOkiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up data and helper functions"
      ],
      "metadata": {
        "id": "23yRiJTQPDmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "DATASET_DIR = \"/content/drive/MyDrive/asr-qcFrench-data/segments\"\n",
        "#DATASET_DIR = \"/content/drive/MyDrive/asr-qcFrench-data/test-input\"\n",
        "dataset_dir = Path(DATASET_DIR) \n",
        "transcripts_path_list = list(dataset_dir.glob(\"*.txt\"))\n",
        "print(len(transcripts_path_list))\n",
        "print(transcripts_path_list)"
      ],
      "metadata": {
        "id": "8l1xAzdlPDQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 16000\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "AUDIO_MAX_LENGTH = 96000 ## TO 6 SEC\n",
        "TEXT_MAX_LENGTH = 1200 ## very big to not filter things\n",
        "SEED = 3407\n",
        "DEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "seed_everything(SEED, workers=True)"
      ],
      "metadata": {
        "id": "d99uGKwjjkvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read wav files\n",
        "def load_wave(wave_path, sample_rate:int=16000) -> torch.Tensor:\n",
        "    waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
        "    if sample_rate != sr:\n",
        "        waveform = at.Resample(sr, sample_rate)(waveform)\n",
        "    return waveform"
      ],
      "metadata": {
        "id": "sQx7Qp9GPUiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# French tokenizer\n",
        "woptions = whisper.DecodingOptions(language=\"fr\", without_timestamps=True)\n",
        "wmodel = whisper.load_model(\"base\")\n",
        "wtokenizer = whisper.tokenizer.get_tokenizer(True, language=\"fr\", task=woptions.task)"
      ],
      "metadata": {
        "id": "lMnuXomXPWBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The audio file list function for the 2.5 hours dataset \n",
        "def get_audio_file_list(transcripts_path_list, text_max_length=120, audio_max_sample_length=96000, sample_rate=16000):\n",
        "    audio_transcript_pair_list = []\n",
        "    for transcripts_path in transcripts_path_list:\n",
        "        with open(transcripts_path, \"r\") as f:\n",
        "            text = f.read()\n",
        "            audio_id = os.path.basename(f.name).rstrip(\".txt\")\n",
        "\n",
        "            audio_path = transcripts_path.with_suffix(\".wav\")\n",
        "            if audio_path.exists():\n",
        "                # Keep the data that satisfy the criteria\n",
        "                audio = load_wave(audio_path, sample_rate=sample_rate)[0]\n",
        "                if len(text) > text_max_length or len(audio) > audio_max_sample_length:\n",
        "                    continue\n",
        "                audio_transcript_pair_list.append((audio_id, str(audio_path), text))\n",
        "    return audio_transcript_pair_list"
      ],
      "metadata": {
        "id": "5RvjIYZdP_1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make audio-transcript pair\n",
        "audio_transcript_pair_list = get_audio_file_list(transcripts_path_list, 120, 96000, 16000)"
      ],
      "metadata": {
        "id": "_IqHcBLiQDGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train, test, val\n",
        "train, test= sklearn.model_selection.train_test_split(audio_transcript_pair_list, test_size=0.2, random_state=1)\n",
        "train, val = sklearn.model_selection.train_test_split(train, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "fWVoJqL7QHLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the split\n",
        "print(\"train\", train[0:10])\n",
        "print(\"TRAIN AUDIO DATASET NUM: \", len(train))\n",
        "print(\"val\", val[0:10])\n",
        "print(\"EVAL AUDIO DATASET NUM: \", len(val))\n",
        "print(\"test\", test[0:10])\n",
        "print(\"TEST AUDIO DATASET NUM: \", len(test))"
      ],
      "metadata": {
        "id": "Ez-xM92QQI7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "class FrSpeechDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_info_list:list, tokenizer, sample_rate) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.audio_info_list = audio_info_list\n",
        "        self.sample_rate = sample_rate\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.audio_info_list)\n",
        "    \n",
        "    def __getitem__(self, id):\n",
        "        audio_id, audio_path, text = self.audio_info_list[id]\n",
        "\n",
        "        # audio\n",
        "        audio = load_wave(audio_path, sample_rate=self.sample_rate)\n",
        "        audio = whisper.pad_or_trim(audio.flatten())\n",
        "        mel = whisper.log_mel_spectrogram(audio)\n",
        "        text_str = [*self.tokenizer.sot_sequence_including_notimestamps] + self.tokenizer.encode(text)\n",
        "        labels = text_str[1:] + [self.tokenizer.eot]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": mel,\n",
        "            \"labels\": labels,\n",
        "            \"dec_input_ids\": text_str\n",
        "        }"
      ],
      "metadata": {
        "id": "FDDqrb5VQLLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for them to be used in the model \n",
        "class WhisperDataCollatorWhithPadding:\n",
        "    def __call__(self, features):\n",
        "        input_ids, labels, dec_input_ids = [], [], []\n",
        "        for f in features:\n",
        "            input_ids.append(f[\"input_ids\"])\n",
        "            labels.append(f[\"labels\"])\n",
        "            dec_input_ids.append(f[\"dec_input_ids\"])\n",
        "\n",
        "        input_ids = torch.concat([input_id[None, :] for input_id in input_ids])\n",
        "        \n",
        "        label_lengths = [len(lab) for lab in labels]\n",
        "        dec_input_ids_length = [len(e) for e in dec_input_ids]\n",
        "        max_label_len = max(label_lengths+dec_input_ids_length)\n",
        "\n",
        "        labels = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(labels, label_lengths)]\n",
        "        dec_input_ids = [np.pad(e, (0, max_label_len - e_len), 'constant', constant_values=50257) for e, e_len in zip(dec_input_ids, dec_input_ids_length)] # 50257 is eot token id\n",
        "\n",
        "        batch = {\n",
        "            \"labels\": labels,\n",
        "            \"dec_input_ids\": dec_input_ids\n",
        "        }\n",
        "        batch = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
        "        batch[\"input_ids\"] = input_ids\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "GV1ESuj5QRJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "train_dataset = FrSpeechDataset(train, wtokenizer, SAMPLE_RATE)\n",
        "val_dataset = FrSpeechDataset(val, wtokenizer, SAMPLE_RATE)\n",
        "test_dataset = FrSpeechDataset(test, wtokenizer, SAMPLE_RATE)"
      ],
      "metadata": {
        "id": "iVcZ--KHQaBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare for loading the different sets\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 2, collate_fn=WhisperDataCollatorWhithPadding())\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 2, collate_fn=WhisperDataCollatorWhithPadding())\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 2, collate_fn=WhisperDataCollatorWhithPadding())"
      ],
      "metadata": {
        "id": "Ctaf9VuVQl0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in train_loader:\n",
        "    print(b[\"labels\"].shape)\n",
        "    print(b[\"input_ids\"].shape)\n",
        "    print(b[\"dec_input_ids\"].shape)\n",
        "\n",
        "    for token, dec in zip(b[\"labels\"], b[\"dec_input_ids\"]):\n",
        "        token[token == -100] = wtokenizer.eot\n",
        "        text = wtokenizer.decode(token, skip_special_tokens=False)\n",
        "        print(text)\n",
        "\n",
        "        dec[dec == -100] = wtokenizer.eot\n",
        "        text = wtokenizer.decode(dec, skip_special_tokens=False)\n",
        "        print(text)\n",
        "    break"
      ],
      "metadata": {
        "id": "6vX4OA1SQsRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the fine-tuning model"
      ],
      "metadata": {
        "id": "4xSqmPZgSF3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    learning_rate = 0.0005 #0.005 #0.01\n",
        "    weight_decay = 0.01\n",
        "    adam_epsilon = 1e-8\n",
        "    warmup_steps = 2\n",
        "    batch_size = 16 # 32\n",
        "    num_worker = 2\n",
        "    num_train_epochs = 10\n",
        "    gradient_accumulation_steps = 1\n",
        "    sample_rate = SAMPLE_RATE"
      ],
      "metadata": {
        "id": "TBndKbBiSFUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WhisperModelModule(LightningModule):\n",
        "    def __init__(self, cfg:Config, model_name=\"small\", lang=\"fr\", train_dataset=[], eval_dataset=[]) -> None:\n",
        "        super().__init__()\n",
        "        self.options = whisper.DecodingOptions(language=lang, without_timestamps=True)\n",
        "        self.model = whisper.load_model(model_name)\n",
        "        self.tokenizer = whisper.tokenizer.get_tokenizer(True, language=\"fr\", task=self.options.task)\n",
        "\n",
        "        # only decoder training\n",
        "        for p in self.model.encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "        self.metrics_wer = evaluate.load(\"wer\")\n",
        "        self.metrics_cer = evaluate.load(\"cer\")\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.__train_dataset = train_dataset\n",
        "        self.__eval_dataset = eval_dataset\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_id):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        labels = batch[\"labels\"].long()\n",
        "        dec_input_ids = batch[\"dec_input_ids\"].long()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            audio_features = self.model.encoder(input_ids)\n",
        "\n",
        "        out = self.model.decoder(dec_input_ids, audio_features)\n",
        "        loss = self.loss_fn(out.view(-1, out.size(-1)), labels.view(-1))\n",
        "        self.log(\"train/loss\", loss, on_step=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_id):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        labels = batch[\"labels\"].long()\n",
        "        dec_input_ids = batch[\"dec_input_ids\"].long()\n",
        "\n",
        "\n",
        "        audio_features = self.model.encoder(input_ids)\n",
        "        out = self.model.decoder(dec_input_ids, audio_features)\n",
        "\n",
        "        loss = self.loss_fn(out.view(-1, out.size(-1)), labels.view(-1))\n",
        "\n",
        "        out[out == -100] = self.tokenizer.eot\n",
        "        labels[labels == -100] = self.tokenizer.eot\n",
        "\n",
        "        o_list, l_list = [], []\n",
        "        for o, l in zip(out, labels):\n",
        "            o = torch.argmax(o, dim=1)\n",
        "            o_list.append(self.tokenizer.decode(o, skip_special_tokens=True))\n",
        "            l_list.append(self.tokenizer.decode(l, skip_special_tokens=True))\n",
        "        cer = self.metrics_cer.compute(references=l_list, predictions=o_list)\n",
        "        wer = self.metrics_wer.compute(references=l_list, predictions=o_list)\n",
        "\n",
        "        self.log(\"val/loss\", loss, on_step=True, prog_bar=True, logger=True)\n",
        "        self.log(\"val/cer\", cer, on_step=True, prog_bar=True, logger=True)\n",
        "        self.log(\"val/wer\", wer, on_step=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return {\n",
        "            \"cer\": cer,\n",
        "            \"wer\": wer,\n",
        "            \"loss\": loss\n",
        "        }\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() \n",
        "                            if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.cfg.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() \n",
        "                            if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, \n",
        "                          lr=self.cfg.learning_rate, \n",
        "                          eps=self.cfg.adam_epsilon)\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=self.cfg.warmup_steps, \n",
        "            num_training_steps=self.t_total\n",
        "        )\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]\n",
        "    \n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            self.t_total = (\n",
        "                (len(self.__train_dataset) // (self.cfg.batch_size))\n",
        "                // self.cfg.gradient_accumulation_steps\n",
        "                * float(self.cfg.num_train_epochs)\n",
        "            )\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        dataset = FrSpeechDataset(self.__train_dataset, self.tokenizer, self.cfg.sample_rate)\n",
        "        return torch.utils.data.DataLoader(dataset, \n",
        "                          batch_size=self.cfg.batch_size, \n",
        "                          drop_last=True, shuffle=True, num_workers=self.cfg.num_worker,\n",
        "                          collate_fn=WhisperDataCollatorWhithPadding()\n",
        "                          )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        dataset = FrSpeechDataset(self.__eval_dataset, self.tokenizer, self.cfg.sample_rate)\n",
        "        return torch.utils.data.DataLoader(dataset, \n",
        "                          batch_size=self.cfg.batch_size, \n",
        "                          num_workers=self.cfg.num_worker,\n",
        "                          collate_fn=WhisperDataCollatorWhithPadding()\n",
        "                          )\n",
        "       "
      ],
      "metadata": {
        "id": "bqH16ixJSJK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "7fy6mbAESqAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_output_dir = \"/content/logs\"\n",
        "check_output_dir = \"/content/artifacts\"\n",
        "\n",
        "train_name = \"whisper\"\n",
        "train_id = \"00001\"\n",
        "\n",
        "model_name = \"small\"\n",
        "lang = \"fr\""
      ],
      "metadata": {
        "id": "_JDGXSOZSphz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean cache\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "model = None\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "xgXwTzwSS2cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = Config()\n",
        "\n",
        "Path(log_output_dir).mkdir(exist_ok=True)\n",
        "Path(check_output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "tflogger = TensorBoardLogger(\n",
        "    save_dir=log_output_dir,\n",
        "    name=train_name,\n",
        "    version=train_id\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=f\"{check_output_dir}/checkpoint\",\n",
        "    filename=\"checkpoint-{epoch:04d}\",\n",
        "    save_top_k=-1 # all model save\n",
        ")\n",
        "\n",
        "callback_list = [checkpoint_callback, LearningRateMonitor(logging_interval=\"epoch\")]\n",
        "model = WhisperModelModule(cfg, model_name, lang, train, val)\n"
      ],
      "metadata": {
        "id": "kowhGX76S5mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    precision=16,\n",
        "    accelerator=DEVICE,\n",
        "    max_epochs=cfg.num_train_epochs,\n",
        "    accumulate_grad_batches=cfg.gradient_accumulation_steps,\n",
        "    logger=tflogger,\n",
        "    callbacks=callback_list, \n",
        "    auto_lr_find = TRUE\n",
        "    #auto_scale_batch_size = TRUE\n",
        ")"
      ],
      "metadata": {
        "id": "0LQ01GhUynib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatic find the best learning rate\n",
        "lr_finder = trainer.tuner.lr_find(model)\n",
        "fig = lr_finder.plot(suggest = True)\n",
        "lr_finder.suggestion()"
      ],
      "metadata": {
        "id": "hN4S8J6Owp1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    learning_rate = 2.7542287033381663e-05 #0.0005 #0.005 #0.01\n",
        "    weight_decay = 0.01\n",
        "    adam_epsilon = 1e-8\n",
        "    warmup_steps = 4 # 2\n",
        "    batch_size = 16 # 32\n",
        "    num_worker = 2\n",
        "    num_train_epochs = 15 #10\n",
        "    gradient_accumulation_steps = 1\n",
        "    sample_rate = SAMPLE_RATE"
      ],
      "metadata": {
        "id": "ZHUmYC1SyWwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model)"
      ],
      "metadata": {
        "id": "VmoVsORRwrIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the model performance throughout training"
      ],
      "metadata": {
        "id": "eoSoDgymUJ7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "2_G2YW1UUHKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir /content/logs"
      ],
      "metadata": {
        "id": "wGNFs-QCT-kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best model"
      ],
      "metadata": {
        "id": "UCjKvI25USWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback.best_model_path"
      ],
      "metadata": {
        "id": "syUFS5z3S7gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/artifacts/checkpoint/checkpoint-epoch=0009.ckpt\""
      ],
      "metadata": {
        "id": "CXuKzgbHS91M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load(checkpoint_path)\n",
        "print(state_dict.keys())\n",
        "state_dict = state_dict['state_dict']"
      ],
      "metadata": {
        "id": "W97GB9yhS_m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_model = WhisperModelModule(cfg)\n",
        "whisper_model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "IHte9nuNTBE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_base = whisper.load_model(\"small\")"
      ],
      "metadata": {
        "id": "rHR3y1Q2TCV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "woptions = whisper.DecodingOptions(language=\"fr\", without_timestamps=True)\n",
        "dataset = FrSpeechDataset(val, wtokenizer, SAMPLE_RATE)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=2, collate_fn=WhisperDataCollatorWhithPadding())\n",
        "\n",
        "refs = []\n",
        "res = []\n",
        "res_b = []\n",
        "for b in tqdm(loader):\n",
        "    input_ids = b[\"input_ids\"].half().cuda()\n",
        "    labels = b[\"labels\"].long().cuda()\n",
        "    with torch.no_grad():\n",
        "        results = whisper_model.model.decode(input_ids, woptions)\n",
        "        results_b = whisper.decode(whisper_base, input_ids, woptions)\n",
        "        for r in results:\n",
        "            res.append(r.text)\n",
        "        for r in results_b:\n",
        "          res_b.append(r.text)\n",
        "        for l in labels:\n",
        "            l[l == -100] = wtokenizer.eot\n",
        "            ref = wtokenizer.decode(l, skip_special_tokens=True)\n",
        "            refs.append(ref)"
      ],
      "metadata": {
        "id": "SVofyvyiTD2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline WER"
      ],
      "metadata": {
        "id": "0xB2YvWpU6Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wer_metrics.compute(references=refs, predictions=res_b)"
      ],
      "metadata": {
        "id": "2gTZrb-XU7hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in zip(refs, res_b):\n",
        "    print(\"-\"*10)\n",
        "    print(k)\n",
        "    print(v)"
      ],
      "metadata": {
        "id": "tEruJTMBU7aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our model's WER"
      ],
      "metadata": {
        "id": "PJB9bsi_TGfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wer_metrics = evaluate.load(\"wer\")\n",
        "wer_metrics.compute(references=refs, predictions=res)"
      ],
      "metadata": {
        "id": "jAkONMvqTG4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in zip(refs, res):\n",
        "    print(\"-\"*10)\n",
        "    print(k)\n",
        "    print(v)"
      ],
      "metadata": {
        "id": "uczMZE-zTJRL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}